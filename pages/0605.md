---
layout: default
title: Les datajournalistes parlent de leurs outils préférés
section: pages
---

Psssss. C’est le bruit de vos données qui se décompressent quand vous ouvrez leur emballage sous vide. Et maintenant ? Qu’est-ce que vous cherchez ? Et quels outils allez-vous utiliser ? Nous avons demandé à des datajournalistes de nous raconter un peu comment ils travaillaient avec des données. Voici ce qu’ils nous ont répondu.

#### The Guardian

Au Guardian Datablog, nous aimons vraiment interagir avec nos lecteurs ; en leur permettant de répliquer rapidement nos projets de datajournalisme, nous leur donnons la possibilité de poursuivre notre travail et parfois de repérer des choses qui nous ont échappé. Alors nous utilisons les outils les plus intuitifs possibles. Nous essayons de choisir ceux que n’importe qui peut maîtriser sans avoir à apprendre un langage de programmation ou suivre une formation coûteuse.

Nous utilisons beaucoup les produits Google pour cette raison. Toutes les bases de données que nous nettoyons et publions sont disponibles en feuilles de calcul Google, permettant à toute personne qui dispose d’un compte Google de télécharger les données, de les importer dans son compte et de créer ses propres graphiques, de trier les données et créer des tableaux croisés dynamiques, ou encore d’importer les données dans un outil de son choix.

Pour les données cartographiques, nous utilisons Google Fusion Tables. Quand nous créons des heatmaps dans Fusion, nous partageons nos fichiers KML afin que nos lecteurs puissent télécharger et concevoir leurs propres cartes – en ajoutant par exemple des couches de données supplémentaires sur la carte originale du Datablog. L’autre avantage de ces outils Google, c’est qu’ils marchent sur toutes les plateformes que nos lecteurs utilisent pour accéder au blog : ordinateurs, smartphones et tablettes.

En plus des feuilles de calcul Google et de Fusion, nous utilisons deux autres outils dans notre travail quotidien. Le premier est Tableau, pour visualiser les bases de données multidimensionnelles ; le second est ManyEyes, pour les analyses de données rapides.

Aucun de ces outils n’est parfait, alors nous sommes toujours en quête de meilleurs outils de visualisation pour mieux satisfaire nos lecteurs.

_Lisa Evans, The Guardian_

#### ProPublica

La possibilité de développer et de déployer des logiciels complexes aussi rapidement qu’un journaliste peut écrire un article est une avancée relativement récente. Avant, tout cela prenait beaucoup plus de temps. Les choses ont changé avec l’avènement de deux frameworks de développement gratuits et open source : Django et Ruby on Rails, tous deux créés vers le milieu des années 2000.

Django, qui est basé sur le langage de programmation Python, a été développé par Adrian Holovaty et une équipe du Lawrence Journal-World, un journal du Kansas. Ruby on Rails a été développé à Chicago par David Heinemeier Hansson et 37Signals, une entreprise de développement d’applications web.

Bien que ces deux frameworks adoptent des approches différentes pour le « patron MVC », ils sont tous les deux excellents et permettent de concevoir rapidement des applications web, même très complexes. Ils simplifient le processus de développement en intégrant des fonctions telles que la création et l’extraction d’éléments d’une base de données ou l’association d’une URL à une partie spécifique du code de l’application, ce qui évite au développeur d’avoir à programmer ces fonctions de base.

Bien qu’il n’y ait encore jamais eu d’enquête officielle sur les pratiques des développeurs d’applications d’information aux États-Unis, il semblerait que la plupart des équipes utilise l’un de ces deux frameworks dans leur travail. Chez ProPublica, nous utilisons Ruby on Rails.

Le développement de services web basés sur le Cloud comme Amazon Web Services permet également d’accélérer le déploiement d’applications web.

À part ça, nous utilisons des outils plutôt classiques pour travailler avec des données : Google Refine et Microsoft Excel pour les nettoyer ; SPSS et R pour l’analyse statistique ; ArcGIS et QGIS pour les informations géographiques ; Git pour gérer le code source ; TextMate, Vim et Sublime Text pour écrire le code ; et un mélange de MySQL, PostgreSQL et SQL Server pour les bases de données. Nous avons conçu notre propre framework JavaScript baptisé « Glass » qui nous aide à développer rapidement des interfaces lourdes en JavaScript.

#### Journalism++

##### Travailler au rythme des rédactions

Cet écosystème technique n’est pas du goût de tout le monde mais c’est celui que je privilégie pour la création de petites applications. Il vous permet de travailler dans un environnement personnalisé et réutilisable rapidement, sans nécessiter une configuration trop fastidieuse.

###### NodeJS

Bien que jeune, Node.js est aujourd’hui l’un des langages de script qui réunit l’une des communautés les plus actives. Implémentées en Javascript, les applications Node sont faciles à écrire, leur code est facile à lire (et donc à partager) et bénéficie d’une multitude de modules qui rendent sa mise en œuvre très rapide.

###### NPM

Pour Node Package Manager. C’est le gestionnaire de packages de Node ; extrêmement facile à intégrer à vos projets, il vous permet en outre de partager vos modules avec la même simplicité. Le must-have de tous développeurs Node.

###### ExpressJS

Framework le plus populaire sur Node, il vous permet de simplifier la création d’un serveur web en acceptant une multitude d’extensions (Less, Stylus, Jade, Twig, etc.).

###### Less et Twitter Bootstrap

Less est un pré-processeur de feuille de style qui vous permet d’écrire du CSS très rapidement. Twitter Bootstrap est une librairie CSS/Javascript qui vous offre une ossature pour le design de vos projets. Twitter Bootstrap étant écrit en Less, il est très facile de personnaliser ses variables et de les importer dans vos feuilles de style Less pour une compilation à la volée. Une combinaison qui vous fait gagner un temps précieux si vous n’êtes pas designer mais que vous aimez les interfaces utilisateurs soignées.

<div class="imageblock">
<div class="content">
<img alt="Des données pour l'autisme" src="../img/desdonneespourlautisme.png"></div>
<div class="title"><em><a href='http://www.desdonneespourlautisme.fr/'>Des Données pour l'Autisme</a>, un projet réalisé avec Twitter Bootstrap (Journalism++ et Damien Brunnon)</em></div>
</div>

##### Mettre sur pied une plateforme

L’une des choses à retenir du milieu de l’innovation (dont le journalisme de données s’inspire), c’est la nécessité de créer des outils qui s’inscrivent dans la durée. Créer une base de données à laquelle d’autres services pourront se greffer est la meilleure manière d’encourager l’innovation et de pérenniser vos projets. Certaines technologies vous facilitent cette tâche.

###### Python + Django

Python est un langage mature, robuste et largement utilisé. Django est un framework MVC qui facilite la création de sites en Python. Ce framework vous permettra de générer sans effort un modèle MVC robuste et polyvalent, une interface d’administration fonctionnelle.

###### Tastypie

Tastypie est un module pour Django qui permet de générer une API Rest très complète en s’appuyant sur le MVC de Django.

##### Les technologies du quotidien

Il est certes bon d’avoir une maîtrise des technologies de développement mais, au quotidien, les journalistes n’ont pas forcément le besoin d’aller aussi loin. Heureusement, les outils ne demandant pas de compétences en code sont de plus en plus nombreux.

###### Datawrapper

Datawrapper est un outil de choix pour les journalistes désireux de produire de la visualisation de données de qualité et à grande vitesse. Conçu pour les journalistes, Datawrapper n’exige aucune connaissance préalable pour être utilisé. Un copier-coller dans l’éditeur de Datawrapper et votre tableau de données se transforme en quelques secondes en un graphique intégrable dans une page web.

###### CartoDB

CartoDB est un service de création de cartes interactives extrêmement puissant. Dans le même style que Google Fusion Table, CartoDB vous permet de télécharger vos jeux de données afin de les visualiser sur une carte. En appliquant à cette visualisation un langage inspiré de CSS, cet outil vous permet d’accéder à un haut de degré de personnalisation afin de créer des cartes à forte valeur ajoutée. Conçu par les hackers/designers de Vizzuality, son interface est en outre l’une des plus belles et des plus ergonomiques parmi les outils de cette famille.

Pour les utilisateurs avancés, cette technologie open source offre également à une API très complète qui permet d’interroger les bases de données mises en ligne à l’aide d’un langage SQL.

_Pierre Romera, Journalism++_

#### Open Knowledge Foundation

Je suis un grand amateur de Python. Python est un magnifique langage de programmation open source qui est facile à lire et à écrire (par exemple, il n’est pas nécessaire de clore chaque ligne par un point-virgule). Mais surtout, Python dispose d’une énorme base d’utilisateurs et offre donc des plugins (appelés packages) pour littéralement tout ce dont vous avez besoin.

Pour moi, Django est un outil rarement utile pour un datajournaliste. C’est un framework d’application web en Python – à savoir un outil pour créer de grosses applications web pilotées par des bases de données. Il est beaucoup trop lourd pour produire de petites infographies interactives.

J’utilise également QGis, un toolkit open source offrant de nombreuses fonctionnalités de SIG utiles pour le datajournaliste qui traite des données géographiques de temps à autre. Si vous avez besoin de convertir des données géospatiales d’un format à un autre, QGis est l’outil idéal. Il peut gérer pratiquement n’importe quel format de géodonnées existant (Shapefiles, KML GeoJSON, etc.). Si vous avez besoin d’exclure certaines régions, QGis peut également s’en charger. De plus, il y a une énorme communauté autour de QGis, et vous trouverez donc des tas de ressources sur le Web.

R a été créé principalement comme un outil de visualisation scientifique. Il est difficile de trouver une méthode de visualisation ou une technique d’analyse de données qui n’est pas déjà incluse dans R. R est un univers à part entière, la Mecque de l’analyse de données visuelles. L’inconvénient, c’est qu’il faut apprendre un langage de programmation de plus, car R utilise son propre langage. Mais une fois que vous en maîtrisez les aspects essentiels, aucun outil n’est aussi puissant que R. Un datajournaliste bien formé à R peut s’en servir pour analyser d’énormes bases de données qui dépassent les limites d’Excel (par exemple, un tableau contenant un million de colonnes).

Ce qui est vraiment pratique avec R, c’est que vous pouvez conserver un « protocole » de ce que vous faites avec les données tout au long du processus – depuis la lecture d’un fichier CSV jusqu’à la création de graphiques. Si les données changent, vous pouvez régénérer le graphique d’un seul clic. Si quelqu’un doute de l’intégrité de votre graphique, vous pouvez lui montrer la source exacte, ce qui permet à n’importe qui de recréer exactement le même (ou de trouver les erreurs que vous avez pu commettre).

NumPy utilisé avec MatPlotLib est en quelque sorte une façon de faire la même chose dans Python. C’est une option à envisager si vous êtes déjà formé à Python. En fait, NumPy et MatPlotLib sont deux exemples de packages Python. Ils peuvent être utilisés pour l’analyse et la visualisation de données, et sont tous deux limités aux visualisations statiques. Ils ne peuvent pas être utilisés pour créer des graphiques interactifs avec des infobulles ou d’autres éléments plus avancés.

Je n’utilise pas MapBox, mais j’ai entendu dire que c’était un excellent outil pour créer des cartes plus sophistiquées basées sur OpenStreetMap. Il permet par exemple de personnaliser le style des cartes (couleurs, légendes, etc.). Il existe également un add-on de MapBox appelé Leaflet. En gros, Leaflet est une librairie JavaScript de cartographie qui permet de basculer facilement entre différents fournisseurs de cartes (OSM, MapBox, Google Maps, Bing, etc.).

RaphaelJS est une librairie de visualisation permettant de travailler avec des figures de base (comme des cercles, des lignes, du texte) et de les animer, d’ajouter des interactions, etc. Il n’y a par exemple pas de diagramme en bâtons préconçu, alors vous devrez dessiner vous-même les rectangles.

Mais l’avantage de RaphaelJS, c’est que tout ce que vous créerez avec fonctionnera également dans Internet Explorer. Ce n’est pas le cas de nombreuses autres librairies de visualisation (géniales) comme d3. Malheureusement, de nombreuses personnes utilisent encore IE et aucune rédaction ne peut se permettre d’ignorer 30 % de ses utilisateurs.

Outre RaphaelJS, il y a également la possibilité de créer une visualisation en Flash pour IE. C’est ce que fait The New York Times. Cela implique de développer chaque application en double.

Je ne suis toujours pas sûr de savoir quel est le « meilleur » processus pour rendre les visualisations sur IE et les navigateurs modernes. Parfois, je trouve que les applications utilisant RaphaelJS sont terriblement lentes sous IE, environ dix fois plus qu’une application Flash équivalente sur un navigateur moderne. Alors Flash peut être une bonne solution de secours si vous voulez offrir des visualisations animées de haute qualité à tous vos utilisateurs.

_Gregor Aisch, Open Knowledge Foundation_

#### Walter Cronkite School of Journalism

L’outil que j’utilise le plus est Excel, car il permet de gérer la majorité des problèmes de JAO et a l’avantage d’être facile à apprendre et accessible à la plupart des journalistes. Quand j’ai besoin de fusionner des tableaux, je me sers généralement d’Access, mais j’exporte ensuite le tableau fusionné dans Excel pour le retravailler. J’utilise ArcMap d’ESRI pour les analyses géographiques ; c’est un outil puissant employé par les agences qui recueillent des données géocodées. TextWrangler est excellent pour examiner des données textuelles avec des mises en page et des délimiteurs bizarres, et permet d’effectuer des recherches et des remplacements complexes avec des expressions courantes. Quand j’ai besoin d’employer des techniques statistiques telles que la régression linéaire, j’utilise SPSS, qui offre une interface intuitive. Pour le « gros œuvre », comme les bases de données comprenant des millions d’entrées et devant être sérieusement filtrées et standardisées, j’utilise les logiciels SAS.

_Steve Doig, Walter Cronkite School of Journalism_

#### The Chicago Tribune

Nos outils de prédilection comprennent Python et Django pour tout ce qui est hacking, scraping et traitement de données, et PostGIS, QGIS et MapBox pour concevoir des applications cartographiques complexes. Pour le moment, R et NumPy avec MatPlotLib s’affrontent pour le titre de meilleur kit d’analyse préliminaire, quoique dernièrement, notre outil préféré soit une production maison : CSVKit. Pratiquement tout ce que nous faisons est déployé sur le Cloud.

_Brian Boyer, The Chicago Tribune_

#### La Nación

À La Nación, nous utilisons :

* Excel pour nettoyer, organiser et analyser des données ;

* le tableur Google pour publier des données et les importer dans des services tels que Google Fusion Tables ou la plate-forme Open Data Junar ;

* Junar pour partager nos données et les intégrer dans nos articles et billets de blog ;

* Tableau Public pour nos visualisations de données interactives ;

* Qlikview, un outil d’informatique décisionnelle très rapide pour analyser et filtrer les grosses bases de données ;

* NitroPDF pour convertir les PDF en fichiers texte et Excel ;

* et Google Fusion Tables pour les visualisations cartographiques.

_Angélica Peralta Ramos, La Nación (Argentina)_

#### Transparência Hacker

Chez Transparência Hacker, une organisation communautaire sans parti pris technique, nous utilisons de nombreux outils et langages de programmation différents. Chaque membre a ses préférences personnelles, et cette grande diversité est à la fois notre force et notre faiblesse. Quelques-uns d’entre nous sont en train de concevoir une distribution Linux Transparência Hacker pour l’analyse de données, que l’on pourra démarrer en live-boot n’importe où. Cette distribution contient des outils et des librairies utiles pour manipuler des données, comme Refine, RStudio et OpenOffice Calc (un outil généralement négligé par les plus technophiles, mais vraiment utile pour les tâches simples). Par ailleurs, nous utilisons énormément Scraperwiki pour concevoir rapidement des prototypes et sauvegarder nos résultats en ligne.

Pour les visualisations de données et les graphiques, beaucoup d’outils ont nos faveurs. Python et NumPy sont très puissants. Quelques membres de la communauté jouent un peu avec R, mais au final, je crois que la majorité de nos projets utilisent encore des librairies de graphisme vectoriel en Javascript comme d3, Flot et RaphaelJS. Enfin, nous avons pas mal expérimenté avec la cartographie, et Tilemill s’est avéré être un outil vraiment intéressant dans ce domaine.

_Pedro Markun, Transparência Hacker_

#### L’Avenir : des projets déclinés au quotidien

Depuis 2012, la rédaction de L’Avenir Huy-Waremme, un journal local wallon, s’est lancée dans le datajournalisme. Un challenge pour une des plus petites rédaction du groupe Les Editions de l’Avenir qui compte 9 titres en presse quotidienne régionale francophone belge.

À des projets d’envergure, nous avons préféré des réalisations faciles à décliner au quotidien. La première réalisation a été l’évolution électorale d’un politicien revenant sur le devant de la scène. Nous avions retrouvé ses résultats dans nos archives papier. C’était avec Datawrapper. Cela nous avait pris quelques heures... pour réaliser une petite courbe. Mais grâce à cela, nous avons pu enrichir un article print.

Ces data au quotidien sont un choix dicté par le temps disponible : il faut continuer à remplir un journal papier et à rebondir sur l’actu pour le site web. Pas question non plus de trop peser sur le budget alloué à la rédaction.

Lors de cette première année, les projets ont principalement tourné autour de la visualisation de données. Les sujets ont été aussi variés que [la production de déchets](http://www.lavenir.net/article/detail.aspx?articleid=DMF20120329_010), la [présentation d’un nouveau camp militaire](http://www.lavenir.net/article/detail.aspx?articleid=DMF20120515_029), la répartition d’aides publiques à des [clubs](http://www.lavenir.net/article/detail.aspx?articleid=DMF20121211_00243105) et [communes](http://www.lavenir.net/article/detail.aspx?articleid=DMF20121215_00245140), [le bilan à mi-saison de clubs de foot](http://www.lavenir.net/article/detail.aspx?articleid=DMF20121212_00243513) ou encore [une saga politico-judiciaire](http://www.lavenir.net/article/detail.aspx?articleid=DMF20120426_026).

La réalisation varie d’une heure à une journée. L’avantage pour le journaliste de travailler au quotidien avec des outils comme Datawrapper, Google Fusion Table, Easel, Thinglink, Dipity ou infogr.am, est de réduire le temps nécessaire aux aspects techniques.

Les projets naissent au cours de la réunion hebdomadaire de la rédaction ou au fil des publications. L’idée est de valoriser les données collectées au cours de reportages, ce qui permet de limiter le travail supplémentaire demandé aux journalistes. Nous préparons donc bien les reportages au préalable (en nous demandant ce qu’il faut comme infos ou illustrations pour le projet web) ou alors nous utilisons des données chiffrées souvent difficiles à exploiter en print.

Grâce à cette expérience, nous pouvons envisager de développer des projets plus importants pour le futur. C’est le cas par exemple avec l’exploitation d’archives officielles sur la réparation pour les dommages de la seconde guerre mondiale, pour laquelle nous collectons, avec les lecteurs du journal, des milliers de documents d’archives mis en forme sur une section spécifique du site.

_Arnaud Wéry, L’Avenir_